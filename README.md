# Masters Project
# Multimodal Fusion of Fine Art, Emotion-Driven Captions, and Music


## Disclaimer

This project **does not include source code** as the original implementation was lost after graduation. This README serves to document the conceptual and technical contributions of the work for future reference and inspiration.

For inquiries or collaboration opportunities, please feel free to reach out to the author.
---
## Overview

This project explores the integration of fine art, emotion-driven captioning, and music generation into a unified system. The aim was to provide users with a platform for expressing and experiencing emotions through visual and auditory mediums, leveraging state-of-the-art AI models and methodologies.

The work represents an interdisciplinary fusion of artificial intelligence, fine arts, and music, offering insights into how these domains can intersect to create new forms of creative expression.

**Author**: Maria Alejandra Rivas Adames  
**Programme**: MSci Computer Science  
**Supervisor**: Dr. Hyung Jin Chang  

---

## Abstract

The system explored the use of machine learning for emotion recognition and music generation based on fine art. It employed multimodal architectures, including convolutional and recurrent neural networks, and state-of-the-art music generation models like AudioLDM. The project aimed to create emotionally resonant captions for artworks and generate relevant music based on these captions, providing a bridge between visual and auditory experiences.

Insights from the project highlight challenges and opportunities in the integration of emotion and AI-generated content, paving the way for future research.

---

## Features

1. **Emotion-Driven Captioning**
   - Fine-tuned AI models to recognize and describe emotions depicted in fine art.
   - Captions include subjective qualities like adjectives and emotive language.

2. **Music Generation**
   - Semantic music generation based on emotion-driven captions.
   - Mapping emotions to musical features such as tempo, key, and mood.

3. **User Interaction**
   - A user-friendly interface for uploading images and customizing captions.
   - Options to adjust musical parameters for personalized outputs.

4. **Real-Time Feedback**
   - Low-latency image recognition and music generation.

---

## System Requirements

The project outlined functional and non-functional requirements to ensure robust system design, including:
- Support for diverse datasets in fine art and music.
- Scalable architecture for future enhancements.
- Compatibility with standard hardware and software environments.


---

## Acknowledgments

Gratitude to  Dr. Hyung Jin Chang, for guidance and support throughout the project.

